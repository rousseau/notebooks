{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rousseau/notebooks/blob/main/neuro_summer_school_strasbourg_2022_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLZ3GWJ3gv0l"
      },
      "source": [
        "# Deep Learning and Dynamical Systems\n",
        "Fran√ßois Rousseau, IMT Atlantique, Brest\n",
        "\n",
        "francois.rousseau@imt-atlantique.fr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJS0XOV0hPf0"
      },
      "source": [
        "\n",
        "This hands-on session is dedicated to image registration and template estimation using PyTorch: \n",
        "\n",
        "1. Load and visualize data (MNIST)\n",
        "2. Image registration (based on VoxelMorph)\n",
        "3. Template estimation\n",
        "4. Implicit image registration (using SIREN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EwKsCrfEGZ4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SidhlYog7mH"
      },
      "source": [
        "## Load and visualize 2D data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGwlkumIEGZ7"
      },
      "source": [
        "### **1**. Download MNIST dataset using Pytorch.\n",
        "\n",
        "We start by looking at 2D MNIST digits before moving to brain MRI data. MNIST comes with the Pytorch framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPL7lPRbLVpZ"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
        "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XINFSgvpLdx3"
      },
      "source": [
        "### **2**. Looking at the data...\n",
        "\n",
        "Let split the data into three datasets: training, validation, testing. Then, data need to be normalized (here between 0 and 1). Once data are loaded, it's always great to visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF29qNgALt7z"
      },
      "outputs": [],
      "source": [
        "# extract all 3s\n",
        "digit = 3\n",
        "\n",
        "# convert to numpy arrays\n",
        "x_train_all = train_data.data.numpy()\n",
        "y_train_all = train_data.targets.numpy()\n",
        "x_test_all = test_data.data.numpy()\n",
        "y_test_all = test_data.targets.numpy()\n",
        "\n",
        "x_train = x_train_all[y_train_all == digit, ...]\n",
        "y_train = y_train_all[y_train_all == digit]\n",
        "x_test = x_test_all[y_test_all == digit, ...]\n",
        "y_test = y_test_all[y_test_all == digit]\n",
        "\n",
        "#%% split train into train and validation\n",
        "\n",
        "nb_val = 1000  # keep 1,000 subjects for validation\n",
        "x_val = x_train[-nb_val:, ...]  # this indexing means \"the last nb_val entries\" of the zeroth axis\n",
        "y_val = y_train[-nb_val:]\n",
        "x_train = x_train[:-nb_val, ...]\n",
        "y_train = y_train[:-nb_val]\n",
        "\n",
        "#%% normalization\n",
        "\n",
        "x_train = x_train.astype('float')/255\n",
        "x_val = x_val.astype('float')/255\n",
        "x_test = x_test.astype('float')/255\n",
        "\n",
        "#%%\n",
        "nb_vis = 5\n",
        "\n",
        "# choose nb_vis sample indexes\n",
        "idx = np.random.choice(x_train.shape[0], nb_vis, replace=False)\n",
        "example_digits = [f for f in x_train[idx, ...]]\n",
        "plt.figure()\n",
        "ax1 = plt.subplot(151)\n",
        "ax1.imshow(example_digits[0], cmap=\"gray\")\n",
        "ax2 = plt.subplot(152)\n",
        "ax2.imshow(example_digits[1], cmap=\"gray\")\n",
        "ax3 = plt.subplot(153)\n",
        "ax3.imshow(example_digits[2], cmap=\"gray\")\n",
        "ax4 = plt.subplot(154)\n",
        "ax4.imshow(example_digits[3], cmap=\"gray\")\n",
        "ax5 = plt.subplot(155)\n",
        "ax5.imshow(example_digits[4], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pzMpuZIEGaB"
      },
      "source": [
        "Some of the most popular models like to have inputs that are sized as multiples of 2^N for N being the number of layers. Here, we force the images to be size 32 (2x 2^4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK97ETNHL0xi"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "\n",
        "pad_amount = ((0, 0), (2,2), (2,2))\n",
        "\n",
        "# fix data\n",
        "x_train = np.pad(x_train, pad_amount, 'constant')\n",
        "x_val = np.pad(x_val, pad_amount, 'constant')\n",
        "x_test = np.pad(x_test, pad_amount, 'constant')\n",
        "\n",
        "# verify\n",
        "print('shape of training data', x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDBiZ0EmmWm_"
      },
      "source": [
        "### **3**. Create a Unet network used for the estimation of the deformation field.\n",
        "Given two images (which we call source and target), the goal is to find the deformation between them. In learning-based methods, we use a network that takes in two images and outputs a dense deformation. This deformation gives us the correspondances between the images, and tells us how to moving the source image to match up with the target image.\n",
        "\n",
        "Note: Registration also includes (or refers to) affine transforms, but we ignore that here.\n",
        "\n",
        "We start by building a model from scratch in order to demonstrate the individual components of the network. \n",
        "\n",
        "First, we're going to define a CNN model (typically UNet) that takes 2 stacked images to estimate a deformation field.\n",
        "\n",
        "Complete the following code accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UPZixOWL3qZ"
      },
      "outputs": [],
      "source": [
        "#%% unet 2d\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_channels = 2, n_classes = 2, n_features = 8):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.n_features = n_features\n",
        "\n",
        "        def double_conv(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        self.dc1 = double_conv(self.n_channels, self.n_features)\n",
        "        self.dc2 = double_conv(self.n_features, self.n_features*2)\n",
        "        self.dc3 = double_conv(self.n_features*2, self.n_features*4)\n",
        "        self.dc4 = double_conv(self.n_features*6, self.n_features*2)\n",
        "        self.dc5 = double_conv(self.n_features*3, self.n_features)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.out = nn.Conv2d(self.n_features, self.n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.dc1(x)\n",
        "\n",
        "        x2 = self.mp(x1)\n",
        "        x2 = self.dc2(x2)\n",
        "\n",
        "        x3 = self.mp(x2)\n",
        "        x3 = self.dc3(x3)\n",
        "\n",
        "        x4 = self.up(x3)\n",
        "        x4 = torch.cat([x4,x2], dim=1)\n",
        "        x4 = self.dc4(x4)\n",
        "\n",
        "        x5 = self.up(x4)\n",
        "        x5 = torch.cat([x5,x1], dim=1)\n",
        "        x5 = self.dc5(x5)\n",
        "        return self.out(x5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHME-RisEGaF"
      },
      "source": [
        "### **4**. Define a spatial transformer used to deform images.\n",
        "Given a deformation field, we need to compute the loss function (i.e. compute a distance between the target and the deformed source image). To this end, we need a module that deforms an image. This is the purpose of the SpatialTransformer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa4WB3KUMBQE"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "# code from voxelmorph repo\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
        "        # adds it to the state dict. this is annoying since everything in the state dict\n",
        "        # is included when saving weights to disk, so the model files are way bigger\n",
        "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
        "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # need to normalize grid values to [-1, 1] for resampler\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "\n",
        "        return F.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EJq1HWnEGaH"
      },
      "source": [
        "### **5**. Using Pytorch lightning, define a network for unsupervised registration.\n",
        "Now, we have all the modules to set up a network dedicated to image registration. Using Pytorch lightning, define such a model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZrScsgpMH5d"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "  \n",
        "import pytorch_lightning as pl\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "class morph_model(pl.LightningModule):\n",
        "  def __init__(self, shape):\n",
        "    super().__init__()   \n",
        "    self.shape = shape\n",
        "    self.unet_model = Unet()\n",
        "    self.transformer = SpatialTransformer(size=shape)\n",
        "\n",
        "  def forward(self,source,target):\n",
        "    # TODO\n",
        "    \n",
        "    return y_source, flow \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    source, target = batch\n",
        "    \n",
        "    # TODO \n",
        "   \n",
        "    return loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnHloLS0EGaJ"
      },
      "source": [
        "### **6**. Define a CustomDataSet dedicated to image registration.\n",
        "To train, we need to make sure the data is in the right format and fed to the model the way we want it such that pytorch models can be trained.\n",
        "\n",
        "Let's code a data generator based on the MNIST data using the CustomDataSet class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd10nedPML5w"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataSet(Dataset):\n",
        "  def __init__(self, X):\n",
        "    self.X = X\n",
        "    self.len = len(self.X)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    index_source = torch.randint(self.len,(1,))\n",
        "    index_target = torch.randint(self.len,(1,))\n",
        "\n",
        "    _source = self.X[index_source][0]\n",
        "    _target = self.X[index_target][0]\n",
        "    \n",
        "    return _source, _target\n",
        "\n",
        "#%%\n",
        "batch_size = 32\n",
        "n_training = x_train.shape[0]\n",
        "source = torch.reshape(torch.Tensor(x_train[:n_training, ...]),(n_training,1,32,32))\n",
        "\n",
        "trainset = CustomDataSet(source)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfXrEAGKEGaK"
      },
      "source": [
        "### **7**. Train the network and visualize the evolution of the loss wrt epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuDbJz3sMicV"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "n_epochs = 10\n",
        "\n",
        "trainer = pl.Trainer(gpus=1, \n",
        "                     max_epochs=n_epochs,\n",
        "                     logger=TensorBoardLogger(save_dir='lightning_logs', default_hp_metric=False, log_graph=True))\n",
        "trainer.fit(net, trainloader)     \n",
        "\n",
        "#%%\n",
        "# TODO (visualization of the loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxNdmDeeEGaL"
      },
      "source": [
        "### **8**. Let's define visualization functions (images and deformation fields)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmXMd_n9EGaL"
      },
      "outputs": [],
      "source": [
        "def visu_img(imgs):\n",
        "  n_imgs = len(imgs)\n",
        "  fig, axs = plt.subplots(1, n_imgs)\n",
        "  if n_imgs == 1:\n",
        "    axs = [axs]    \n",
        "  for i in range(n_imgs):\n",
        "    axs[i].imshow(imgs[i], interpolation=\"nearest\", cmap=\"gray\")    \n",
        "  plt.show()\n",
        "\n",
        "def visu_flow(flows):\n",
        "  n_flows = len(flows)\n",
        "  fig, axs = plt.subplots(1, n_flows)\n",
        "  if n_flows == 1:\n",
        "    axs = [axs]    \n",
        "  for i in range(n_flows):\n",
        "    u, v = flows[i][0,0,:,:],flows[i][0,1,:,:]\n",
        "    axs[i].quiver(u,v)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTeMoZ9EGaM"
      },
      "source": [
        "### **9**. Apply the trained network and visualize the deformed image (and the deformation field).\n",
        "\n",
        "With pair-wise optimization methods (like most classical methods), to register a new pair you would need to optimize a deformation field.\n",
        "\n",
        "With learning based registration, we simply evaluate the network for a new input pair.\n",
        "\n",
        "Applies the trained network on two images (same digit: 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul8-Xcy3NciB"
      },
      "outputs": [],
      "source": [
        "n_source = 0\n",
        "n_target = 6\n",
        "source_np = x_train[n_source, ...]\n",
        "target_np = x_train[n_target, ...]\n",
        "\n",
        "source = torch.reshape(torch.Tensor(source_np),(1,1,32,32))\n",
        "target =  torch.reshape(torch.Tensor(target_np),(1,1,32,32))\n",
        "\n",
        "y_source, flow = net.forward(source,target)\n",
        "y_source_np = np.reshape(y_source.cpu().detach().numpy(),(32,32))\n",
        "flow_np = flow.cpu().detach().numpy()\n",
        "\n",
        "visu_img([source_np, target_np, y_source_np])\n",
        "visu_flow([flow_np])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjE78CfxEGaO"
      },
      "source": [
        "### **10**. What about generalization? Apply the trained networt on a different digit class.\n",
        "\n",
        "How do learning-based methods generalize beyond training distribution ?\n",
        "\n",
        "An important caveat to learning-based registration is that they will, in general, only register samples fromt he distribution they've been trained from.\n",
        "\n",
        "So, what happens if we register two 5's?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ZpqAmrNgvc"
      },
      "outputs": [],
      "source": [
        "#%% Generalization on another digit\n",
        "digit = 5\n",
        "\n",
        "x_fives = x_train_all[y_train_all == digit, ...]\n",
        "x_fives = np.pad(x_fives, pad_amount, 'constant')\n",
        "\n",
        "n_source = 0\n",
        "n_target = 2\n",
        "source = torch.reshape(torch.Tensor(x_fives[n_source, ...]),(1,1,32,32))\n",
        "target =  torch.reshape(torch.Tensor(x_fives[n_target, ...]),(1,1,32,32))\n",
        "\n",
        "# TODO : apply the network on (source,target) and visualize the warped image and the estimated flow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GvRJpg7EGaQ"
      },
      "source": [
        "### **11**. Modify the registration network by introducing stationary velocity fields for transform modeling.\n",
        "\n",
        "The deformation field is the integration of a stationary velocity field. To integrate the velocity, we make use of the VecInt class from VoxelMorph library.\n",
        "\n",
        "Define a SVF model by modifying the way the deformation field is computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbnMuZqmNlJ1"
      },
      "outputs": [],
      "source": [
        "#%% Diffeomorphism (SVF)\n",
        "\n",
        "#from voxelmorph repo\n",
        "class VecInt(nn.Module):\n",
        "    \"\"\"\n",
        "    Integrates a vector field via scaling and squaring.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inshape, nsteps):\n",
        "        super().__init__()\n",
        "\n",
        "        assert nsteps >= 0, 'nsteps should be >= 0, found: %d' % nsteps\n",
        "        self.nsteps = nsteps\n",
        "        self.scale = 1.0 / (2 ** self.nsteps)\n",
        "        self.transformer = SpatialTransformer(inshape)\n",
        "\n",
        "    def forward(self, vec):\n",
        "        vec = vec * self.scale\n",
        "        for _ in range(self.nsteps):\n",
        "            vec = vec + self.transformer(vec, vec)\n",
        "        return vec\n",
        "\n",
        "\n",
        "class SVF_model(pl.LightningModule):\n",
        "  def __init__(self, shape, int_steps = 7):\n",
        "    super().__init__()   \n",
        "    self.shape = shape\n",
        "    self.unet_model = Unet()\n",
        "    self.transformer = SpatialTransformer(size=shape)\n",
        "    self.int_steps = int_steps #number of integration step (i.e. flow is integrated from velocity fields). \n",
        "    self.vecint = VecInt(inshape=shape, nsteps=int_steps)\n",
        "    \n",
        "  def forward(self,source,target):\n",
        "\n",
        "    #TODO\n",
        "        \n",
        "    return y_source, flow \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    source, target = batch\n",
        "\n",
        "    #TODO \n",
        "     \n",
        "    return loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NqECfdCEGaR"
      },
      "source": [
        "### **12**. Study the performances of the SVF network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e787zxHtNqJi"
      },
      "outputs": [],
      "source": [
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u57kUzLBEGaS"
      },
      "source": [
        "### **13**. Extend the SVF network to deal with bidirectional flows.\n",
        "\n",
        "SVF-based modeling is a way to build diffeomorphisms (an invertible function that maps one differentiable manifold to another such that both the function and its inverse are smooth). We can make use of this property to estimate the backward deformation field by integrating the backward velocity field (which can be defined as the opposite of the forward velocity field).\n",
        "\n",
        "Modify the previous network to introduce forward and backward deformation fields into the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7DWPekpNvjg"
      },
      "outputs": [],
      "source": [
        "#%% Bidirectional registration\n",
        "class SVF_bidir_model(pl.LightningModule):\n",
        "  def __init__(self, shape, int_steps = 7):\n",
        "    super().__init__()   \n",
        "    self.shape = shape\n",
        "    self.unet_model = Unet()\n",
        "    self.transformer = SpatialTransformer(size=shape)\n",
        "    self.int_steps = int_steps #number of integration step (i.e. flow is integrated from velocity fields). \n",
        "    self.vecint = VecInt(inshape=shape, nsteps=int_steps)\n",
        "    \n",
        "  def forward(self,source,target):\n",
        "\n",
        "    # TODO\n",
        "    \n",
        "    return y_source, y_target \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    source, target = batch\n",
        "\n",
        "    #TODO\n",
        "    \n",
        "    return loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIht9nukEGaT"
      },
      "source": [
        "### **14**. Study of the bidirectional SVF network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_9rwXQQN9U2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPYct-sUEGaU"
      },
      "source": [
        "## Atlas building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPvi0fOAEGaV"
      },
      "source": [
        "### **1**. Define a network for atlas building.\n",
        "\n",
        "Modify the previous bidirectional SVF model by adding: 1) a learnable image (i.e. the template), 2) a loss related to definition of a template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTUc_HbUOCfL"
      },
      "outputs": [],
      "source": [
        "class atlas_building_model(pl.LightningModule):\n",
        "  def __init__(self, shape, int_steps = 7, init_atlas = None):\n",
        "    super().__init__()\n",
        "    \n",
        "    # TODO\n",
        "    \n",
        "  def forward(self,source):\n",
        "\n",
        "    # TODO\n",
        "    \n",
        "    return y_source, y_target, forward_flow \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    # TODO\n",
        "    \n",
        "    return loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyQ-6wp__DcY"
      },
      "source": [
        "Instantiate an atlas building model and visualize the initial template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amPC3pLMtBDI"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCfpVxOtEGaW"
      },
      "source": [
        "### **2**. Train the atlas building network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe000Q5EQ_PS"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud3BMNLeEGaX"
      },
      "source": [
        "### **3**. Visualise the estimated atlas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzgW-KvedhYg"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWPaJMoFEGaY"
      },
      "source": [
        "### **4**. Application to 2D MRI data.\n",
        "\n",
        "We will now register slightly more realistic data - MRIs of the brain. To be able to train and easily register during this tutorial, we will first extract the middle slice of brain scans.\n",
        "\n",
        "Note that because this task does not capture deformations in the third dimensions, certain correspondances are not exactly possible. Nonetheless, this exercise will illustrate registration with more realistic complex images.\n",
        "\n",
        "The brains have been intensity-normalized affinely aligned, and skull-stripped with FreeSurfer, to enable focusing on deformable registration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cup_SjHyx45H"
      },
      "outputs": [],
      "source": [
        "# download MRI tutorial data\n",
        "!wget https://surfer.nmr.mgh.harvard.edu/pub/data/voxelmorph/tutorial_data.tar.gz -O data.tar.gz\n",
        "!tar -xzvf data.tar.gz\n",
        "npz = np.load('tutorial_data.npz')\n",
        "\n",
        "brains = npz['train']\n",
        "\n",
        "vol_shape = brains.shape[1:]\n",
        "print('train shape:', brains.shape)\n",
        "\n",
        "visu_img([brains[0,...],brains[1,...],brains[3,...]])\n",
        "\n",
        "n_training = brains.shape[0]\n",
        "source = torch.reshape(torch.Tensor(brains[:n_training, ...]),(n_training,1,192,160))\n",
        "trainset = CustomDataSet(source)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gTB2Mk2Vj6p"
      },
      "source": [
        "Setup a model and visualize the initial template estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI16ml2FyNF0"
      },
      "outputs": [],
      "source": [
        "brain_atlas_building_net = atlas_building_model(shape=(192,160))#, init_atlas = mean_brain)\n",
        "\n",
        "template = brain_atlas_building_net.atlas\n",
        "print(template.shape)\n",
        "plt.figure()\n",
        "plt.imshow(template[0,...].cpu().detach().numpy(),cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULIfnlbVtNa"
      },
      "source": [
        "Run the training step and visualize the estimated template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QffC6yPvyoSj"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDbhGiZcEGab"
      },
      "source": [
        "## Implicit representation of the deformation fields for pairwise registration\n",
        "\n",
        "We investigate in this section the use of neural fields (i.e. implicit representation) of the deformation fields for image registration. For each new image pair, we optimize a network that takes as input any spatial coordinate and provide as output a function value. \n",
        "\n",
        "Let first define a SIREN network.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVTssg0vEGac"
      },
      "outputs": [],
      "source": [
        "#%% Code from SIREN repo modified for lightning\n",
        "import math\n",
        "!pip install einops\n",
        "from einops import rearrange\n",
        "\n",
        "def exists(val):\n",
        "  return val is not None\n",
        "\n",
        "def cast_tuple(val, repeat = 1):\n",
        "  return val if isinstance(val, tuple) else ((val,) * repeat)\n",
        "  \n",
        "class Sine(nn.Module):\n",
        "  def __init__(self, w0 = 1.):\n",
        "    super().__init__()\n",
        "    self.w0 = w0\n",
        "  def forward(self, x):\n",
        "    return torch.sin(self.w0 * x)\n",
        "\n",
        "# siren layer\n",
        "class Siren(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out, w0 = 1., c = 6., is_first = False, use_bias = True, activation = None):\n",
        "    super().__init__()\n",
        "    self.dim_in = dim_in\n",
        "    self.is_first = is_first\n",
        "\n",
        "    weight = torch.zeros(dim_out, dim_in)\n",
        "    bias = torch.zeros(dim_out) if use_bias else None\n",
        "    self.init_(weight, bias, c = c, w0 = w0)\n",
        "\n",
        "    self.weight = nn.Parameter(weight)\n",
        "    self.bias = nn.Parameter(bias) if use_bias else None\n",
        "    self.activation = Sine(w0) if activation is None else activation\n",
        "\n",
        "  def init_(self, weight, bias, c, w0):\n",
        "    dim = self.dim_in\n",
        "\n",
        "    w_std = (1 / dim) if self.is_first else (math.sqrt(c / dim) / w0)\n",
        "    weight.uniform_(-w_std, w_std)\n",
        "\n",
        "    if exists(bias):\n",
        "        bias.uniform_(-w_std, w_std)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out =  F.linear(x, self.weight, self.bias)\n",
        "    out = self.activation(out)\n",
        "    return out\n",
        "\n",
        "# siren network\n",
        "class SirenNet(nn.Module):\n",
        "  def __init__(self, dim_in=2, dim_hidden=128, dim_out=2, num_layers=2, w0 = 1., w0_initial = 30., use_bias = True, final_activation = None):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.dim_hidden = dim_hidden\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for ind in range(num_layers):\n",
        "        is_first = ind == 0\n",
        "        layer_w0 = w0_initial if is_first else w0\n",
        "        layer_dim_in = dim_in if is_first else dim_hidden\n",
        "\n",
        "        self.layers.append(Siren(\n",
        "            dim_in = layer_dim_in,\n",
        "            dim_out = dim_hidden,\n",
        "            w0 = layer_w0,\n",
        "            use_bias = use_bias,\n",
        "            is_first = is_first\n",
        "        ))\n",
        "\n",
        "    final_activation = nn.Identity() if not exists(final_activation) else final_activation\n",
        "    self.last_layer = Siren(dim_in = dim_hidden, dim_out = dim_out, w0 = w0, use_bias = use_bias, activation = final_activation)\n",
        "\n",
        "  def forward(self, x, mods = None):\n",
        "    mods = cast_tuple(mods, self.num_layers)\n",
        "\n",
        "    for layer, mod in zip(self.layers, mods):\n",
        "      x = layer(x)\n",
        "\n",
        "      if exists(mod):\n",
        "        x *= rearrange(mod, 'd -> () d')\n",
        "\n",
        "    return self.last_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7l8a5YJEGad"
      },
      "source": [
        "This SIREN network is now added in a registration module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbZ0At7uEGad"
      },
      "outputs": [],
      "source": [
        "class siren_morph_model(pl.LightningModule):\n",
        "  def __init__(self, shape):\n",
        "    super().__init__()   \n",
        "    self.shape = shape\n",
        "    self.siren_model = SirenNet()\n",
        "    self.transformer = SpatialTransformer(size=shape)\n",
        "    \n",
        "    #create 2d grid\n",
        "    x = torch.linspace(-1, 1, steps=self.shape[0])\n",
        "    y = torch.linspace(-1, 1, steps=self.shape[1])\n",
        "    mgrid = torch.stack(torch.meshgrid(x,y), dim=-1)\n",
        "    self.grid = torch.Tensor(mgrid.reshape(-1,2))\n",
        "\n",
        "  def forward(self,source,target):\n",
        "\n",
        "    # TODO\n",
        "    \n",
        "    return y_source, flow \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    #TODO \n",
        "    \n",
        "    return loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDWBOKF2EGae"
      },
      "source": [
        "Since continuous representations rely on coordinate-based neural network, a training step is required for each new pair of images. Define a custom dataset containing only two images and train the registration network based on SIREN modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqi_tYOgEGae"
      },
      "outputs": [],
      "source": [
        "class TwoDataSet(Dataset):\n",
        "  def __init__(self, X):\n",
        "    self.X = X\n",
        "    self.len = len(self.X)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    index_source = 0\n",
        "    index_target = 1\n",
        "\n",
        "    _source = self.X[index_source]\n",
        "    _target = self.X[index_target]\n",
        "    \n",
        "    return _source, _target   \n",
        "\n",
        "n_source = 0\n",
        "n_target = 10\n",
        "\n",
        "source_np = x_train[n_source, ...]\n",
        "target_np = x_train[n_target, ...]\n",
        "\n",
        "source = torch.reshape(torch.Tensor(source_np),(1,1,32,32))\n",
        "target =  torch.reshape(torch.Tensor(target_np),(1,1,32,32))\n",
        "\n",
        "x_train_reg = torch.cat([source,target],dim=0)\n",
        "batch_size_reg = 1\n",
        "trainset_reg = TwoDataSet(x_train_reg)\n",
        "trainloader_reg = torch.utils.data.DataLoader(trainset_reg, batch_size=batch_size_reg)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2nIZQ-2V1X-"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g2dQmyGEGae"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mshCUFkPV6yK"
      },
      "source": [
        "Visualize the images (source, target, warped source) and the corresponding deformation fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM8sdQQREGaf"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApngUXmcEGaf"
      },
      "source": [
        "Implicit representation can be used to add constraints on the estimated deformation fields, such as jacobian regularizer without introducing numerical errors.\n",
        "\n",
        "$$\n",
        "    \\mathcal{L}_{jac}(\\Phi) = \\int_{\\Omega} | 1 - \\det \\nabla \\Phi | d{\\bf x}\n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL1PHp5SEGaf"
      },
      "outputs": [],
      "source": [
        "#Modified code from https://github.com/MIAGroupUT/IDIR\n",
        "def gradient(input_coords, output, grad_outputs=None):\n",
        "    \"\"\"Compute the gradient of the output wrt the input.\"\"\"\n",
        "\n",
        "    grad_outputs = torch.ones_like(output)\n",
        "    grad = torch.autograd.grad(\n",
        "        output, [input_coords], grad_outputs=grad_outputs, create_graph=True\n",
        "    )[0]\n",
        "    return grad\n",
        "\n",
        "def compute_jacobian_matrix(input_coords, output, add_identity=True):\n",
        "    \"\"\"Compute the Jacobian matrix of the output wrt the input.\"\"\"\n",
        "\n",
        "    jacobian_matrix = torch.zeros(input_coords.shape[0], 2, 2)\n",
        "    for i in range(2):\n",
        "        jacobian_matrix[:, i, :] = gradient(input_coords, output[:, i])\n",
        "        if add_identity:\n",
        "            jacobian_matrix[:, i, i] += torch.ones_like(jacobian_matrix[:, i, i])\n",
        "    return jacobian_matrix\n",
        "\n",
        "def compute_jacobian_loss(input_coords, output, batch_size=None):\n",
        "    \"\"\"Compute the jacobian regularization loss.\"\"\"\n",
        "\n",
        "    # Compute Jacobian matrices\n",
        "    jac = compute_jacobian_matrix(input_coords, output)\n",
        "\n",
        "    # Compute determinants and take norm\n",
        "    loss = torch.det(jac) - 1\n",
        "    loss = torch.linalg.norm(loss, 1)\n",
        "\n",
        "    return loss / batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCRtloAbWGNw"
      },
      "source": [
        "Modify the implicit registration model to add a jacobian-based loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d19zSMLNEGag"
      },
      "outputs": [],
      "source": [
        "class siren_reg_morph_model(pl.LightningModule):\n",
        "  def __init__(self, shape):\n",
        "    super().__init__()   \n",
        "    self.shape = shape\n",
        "    self.siren_model = SirenNet()\n",
        "    self.transformer = SpatialTransformer(size=shape)\n",
        "    self.alpha = 0.00001\n",
        "    \n",
        "    #TODO\n",
        "    \n",
        "  def forward(self,source,target):\n",
        "\n",
        "    #TODO\n",
        "    \n",
        "    return y_source, flow \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    #TODO\n",
        "    \n",
        "    return loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbNO54ZRWMoW"
      },
      "source": [
        "Do the training and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t1EZqFJEGag"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "neuro_summer_school_strasbourg_2022_student.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}